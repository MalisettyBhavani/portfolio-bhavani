<div class="user-details">
  <h1>Resume</h1>
</div>

<div id="summary">
  <h3>Summary</h3>
  <strong>AI and Machine Learning Engineer</strong> with specialized expertise in Large Language Models (LLMs), backend engineering, and scalable AI systems. Experienced in building end-to-end LLM pipelines, fine-tuning transformer architectures, and deploying production-ready applications that improve information accessibility, automation, and decision-making.
</div>

<div id="education">
  <h3>Education</h3>

  <p>
    <strong style="display:flex;justify-content:space-between">University of Nebraska at Omaha<span>Omaha, NE</span></strong>
    <strong style="display:flex;justify-content:space-between">Master’s in Computer Science<span>Aug 2023 – May 2025</span></strong>
    <strong>Relevant Coursework:</strong> Advanced Programming Languages, Advanced Operating Systems, Advanced Artificial Intelligence, Cybersecurity, Database Management Systems, Data Warehousing and Mining, Design and Analysis of Algorithms, Software Engineering.
  </p>

  <p>
    <strong style="display:flex;justify-content:space-between">CMR Institute of Technology<span>Hyderabad, India</span></strong>
    <strong style="display:flex;justify-content:space-between">Bachelor’s in Computer Science and Engineering<span>Aug 2018 – May 2022</span></strong>
    <strong>Relevant Coursework:</strong> Data Structures, Machine Learning and Data Science, Software Design and Engineering, Java, Python Programming.
  </p>
</div>

<div id="work-experience">
  <h3>Professional Experience</h3>

  <section class="role">
    <strong style="display:flex;justify-content:space-between">
      University of Nebraska at Omaha<span>Omaha, NE</span>
    </strong>
    <strong style="display:flex;justify-content:space-between">
      LLM Engineer Intern<span>Aug 2025 – Present</span>
    </strong>
    <ul>
      <li>Built an AI-powered question-answering system using LLM inference with Retrieval-Augmented Generation for lecture notes and videos.</li>
      <li>Designed a video intelligence pipeline integrating ASR, NLP preprocessing, and semantic embeddings for content retrieval.</li>
      <li>Fine-tuned vision-language and transformer models, improving QA accuracy by 20% on internal benchmarks.</li>
      <li>Implemented dense retrieval with optimized embeddings to improve relevance while reducing latency.</li>
      <li>Developed a real-time interface in React, WebSockets, and FastAPI for low-latency interaction.</li>
      <li>Applied prompt engineering, embedding refinement, and guardrails to ensure safe, reliable outputs.</li>
      <li>Added monitoring and observability for LLM pipelines; explored compression and mixed-precision for efficient inference.</li>
    </ul>
  </section>

  <section class="role">
    <strong style="display:flex;justify-content:space-between">
      University of Nebraska at Omaha<span>Omaha, NE</span>
    </strong>
    <strong style="display:flex;justify-content:space-between">
      Applied Research Scientist – Language Models<span>Aug 2023 – May 2025</span>
    </strong>
    <ul>
      <li>Evaluated quantized LLaMA-2 models for IoT privacy-policy language generation, measuring latency, throughput, and quality trade-offs.</li>
      <li>Engineered structured prompts that improved next-token prediction accuracy by 15% on incomplete policy statements.</li>
      <li>Developed a multi-label classifier with baseline, fine-tuned, and quantized LLaMA-2 variants using supervised fine-tuning, achieving a 12% accuracy improvement on OPP-115.</li>
      <li>Built a React and WebSockets tool for real-time sentence classification, paraphrasing, and category-level visualization.</li>
      <li>Established responsible evaluation and interpretability practices for LLM outputs.</li>
      <li>Investigated quantization and mixed-precision strategies to reduce memory footprint and inference costs.</li>
      <li>Assessed scalability and integration patterns for production-like environments and high-volume queries.</li>
    </ul>
  </section>

  <section class="role">
    <strong style="display:flex;justify-content:space-between">
      Capgemini Engineering<span>Bangalore, India</span>
    </strong>
    <strong style="display:flex;justify-content:space-between">
      Software Engineer<span>Jan 2022 – Jul 2023</span>
    </strong>
    <ul>
      <li>Designed and deployed Django REST APIs integrating Oracle E-Business Suite, increasing system throughput by 18%.</li>
      <li>Refactored monolithic services into modular, asynchronous workflows; ORM tuning and SQL indexing reduced average request latency by 12%.</li>
      <li>Automated backend processes with Python and SQL, lowering manual effort and error rates.</li>
      <li>Implemented CI/CD with Git and Jenkins to improve release reliability and developer velocity.</li>
      <li>Optimized Procure-to-Pay and Order-to-Cash paths for performance, maintainability, and monitoring.</li>
      <li>Created real-time reporting with Django and Jinja2 templates for operational visibility.</li>
    </ul>
  </section>
</div>

<div id="publications">
  <h3>Publications</h3>

  <p>
    <strong>Evaluating Quantized LLaMA 2 Models for IoT Privacy Policy Language Generation </strong><a href="https://www.mdpi.com/1999-5903/16/7/224" target="_blank" rel="noopener">Read the paper</a></br>

    <em>Bhavani Malisetty, Alfredo J. Perez. Future Internet, MDPI, 16(7):224, 2024.</em>
    <ul>
      <li>Assessed 4-bit, 5-bit, and 8-bit quantized LLaMA-2 models and the base model for modeling and generating IoT privacy-policy language.</li>
      <li>Benchmarked with ROUGE, BERTScore, GloVe, and Word2Vec; demonstrated that structured prompts improved next-token prediction by 15% while reducing compute and memory requirements for resource-constrained deployments.</li>
    </ul>
  </p>

  <p>
    <strong>Improving the Understanding of Privacy Policies Using Large Language Models (LLMs) </strong><a href="https://www.proquest.com/openview/9769104cd04428b2c259f5fc2a09a83f/1?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank" rel="noopener">View the thesis</a><br/>
    <em>Bhavani Malisetty. University of Nebraska at Omaha, 2025.</em>
    <ul>
      <li>Developed a multi-label classification framework using baseline, fine-tuned, and quantized LLaMA-2 models, achieving a 12% accuracy improvement on OPP-115 via supervised fine-tuning.</li>
      <li>Built a web interface in React and WebSockets for real-time sentence-level classification, paraphrasing, and category visualization, supporting clearer and more accessible policy communication.</li>
    </ul>
  </p>
</div>

<div id="skills-and-interests">
  <h3>Skills</h3>
  <ul>
    <li><strong>Languages:</strong> Python, Java, JavaScript, C, C++, C#, TypeScript, PL/SQL</li>
    <li><strong>Frameworks & Libraries:</strong> React, Angular, Django, .NET, LangChain, Streamlit, Bootstrap</li>
    <li><strong>Cloud & Containers:</strong> AWS, Docker, Kubernetes, Podman, Flockport</li>
    <li><strong>Databases:</strong> MySQL, Oracle</li>
    <li><strong>Expertise:</strong> Machine Learning, Deep Learning, LLM Fine-Tuning, REST APIs, WebSockets, Git, Linux</li>
  </ul>
</div>

<div id="achievements-and-certifications">
  <h3>Achievements & Certifications</h3>
  <ul>
    <li>Published peer-reviewed research on LLMs in <em>Future Internet (MDPI)</em> and authored a master’s thesis on privacy-policy analysis with LLMs.</li>
    <li>Developed award-winning AI applications, including a GPT-based travel planner and an interactive privacy-policy classification tool.</li>
    <li>Completed certifications in Generative AI (AWS), Machine Learning (Stanford University), and Deep Learning (DeepLearning.AI).</li>
  </ul>
</div>
